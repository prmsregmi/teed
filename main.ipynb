{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating real image based samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s][ WARN:0@0.177] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating improved synthetic samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:00<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not place object within constraints after 100 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [00:01<00:04,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not place object within constraints after 100 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf data/synthetic_train\n",
    "from synthetic_data import generate_synthetic\n",
    "generate_synthetic(10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Data augmentation just for 720x1280 image size ==============\n",
      "Image augmentation by splitting up have started!\n",
      "Directories already exists:  data/synthetic_train/BIPED/edges/imgs/train/rgbr/aug\n",
      "Directories already exists:  data/synthetic_train/BIPED/edges/edge_maps/train/rgbr/aug\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/synthetic_train/BIPED/edges/imgs/train/rgbr/aug/real'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#data augmentation\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_augmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_augmentation\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mrun_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/synthetic_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teed/data_augmentation/main.py:71\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(dataset_dir, rotation)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m(dataset_dir, rotation = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Data augmentation\u001b[39;00m\n\u001b[32m     70\u001b[39m     augment_both = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# to augment the RGB and target (edge_map) image at the same time\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[43maugment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_both\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment_both\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_all_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Data augmentation list maker>>> train_pair.lst\u001b[39;00m\n\u001b[32m     74\u001b[39m     list_data(dataset_dir,\u001b[33m\"\u001b[39m\u001b[33mBIPED\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teed/data_augmentation/data_augmentation.py:664\u001b[39m, in \u001b[36maugment_data\u001b[39m\u001b[34m(base_dir, augment_both, use_all_type, rotation)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m splitting_up:\n\u001b[32m    663\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImage augmentation by splitting up have started!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m     dataset_dirs = \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43maugment_both\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment_both\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m     splitting_up =\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m splitting_up:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teed/data_augmentation/data_augmentation.py:382\u001b[39m, in \u001b[36msplit_data\u001b[39m\u001b[34m(data_dir, augment_both)\u001b[39m\n\u001b[32m    379\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(gt_list) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x_list) == \u001b[38;5;28mlen\u001b[39m(gt_list) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;66;03m# real folder copy to aug dir\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_aug_dir\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/real\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment_both:\n\u001b[32m    384\u001b[39m     shutil.copytree(os.path.join(gt_dir, \u001b[33m'\u001b[39m\u001b[33mreal\u001b[39m\u001b[33m'\u001b[39m), gt_aug_dir+\u001b[33m'\u001b[39m\u001b[33m/real\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/shutil.py:600\u001b[39m, in \u001b[36mcopytree\u001b[39m\u001b[34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m os.scandir(src) \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[32m    599\u001b[39m     entries = \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_copytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43msymlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/shutil.py:498\u001b[39m, in \u001b[36m_copytree\u001b[39m\u001b[34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     ignored_names = ()\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m errors = []\n\u001b[32m    500\u001b[39m use_srcentry = copy_function \u001b[38;5;129;01mis\u001b[39;00m copy2 \u001b[38;5;129;01mor\u001b[39;00m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileExistsError\u001b[39m: [Errno 17] File exists: 'data/synthetic_train/BIPED/edges/imgs/train/rgbr/aug/real'"
     ]
    }
   ],
   "source": [
    "#data augmentation\n",
    "from data_augmentation import run_augmentation\n",
    "run_augmentation('data/synthetic_train', rotation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teed import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import cv2\n",
    "\n",
    "# def validate_images(texture_dir):\n",
    "#     texture_dir = Path(texture_dir)\n",
    "#     texture_files = list(texture_dir.glob('*'))\n",
    "#     valid_images = []\n",
    "#     invalid_images = []\n",
    "\n",
    "#     for file in texture_files:\n",
    "#         img = cv2.imread(str(file))\n",
    "#         if img is None:\n",
    "#             print(f\"Invalid image: {file}\")\n",
    "#             invalid_images.append(file)\n",
    "#         else:\n",
    "#             valid_images.append(file)\n",
    "    \n",
    "#     return valid_images, invalid_images\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     texture_directory = \"data/textures\"\n",
    "#     valid, invalid = validate_images(texture_directory)\n",
    "#     print(f\"Valid images count: {len(valid)}\")\n",
    "#     print(f\"Invalid images count: {len(invalid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "def call_ods_ois(model, mat_dir):\n",
    "    # Path to the external Python interpreter if different from the main project's\n",
    "    external_python = \".venv/bin/python3\"\n",
    "    \n",
    "    # Build the command\n",
    "    cmd = [\n",
    "        external_python,\n",
    "        \"main.py\",\n",
    "        \"--alg\", \"TEED \" + model,\n",
    "        \"--model_name_list\", model,\n",
    "        \"--result_dir\", \"../../\" + mat_dir,\n",
    "        \"--save_dir\", \"../../\" + mat_dir + \"/eval_output\",\n",
    "        \"--gt_dir\", \"../../data/UDED/gt_mat\",\n",
    "        \"--key\", \"result\",\n",
    "        \"--file_format\", \".mat\",\n",
    "        \"--workers\", \"-1\"\n",
    "    ]\n",
    "    \n",
    "    # Call the script, changing to its directory\n",
    "    subprocess.run(cmd, cwd=\"external/edge_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pramesh/teed/external/edge_eval/main.py\", line 5, in <module>\n",
      "    from eval_edge import eval_edge\n",
      "  File \"/home/pramesh/teed/external/edge_eval/eval_edge.py\", line 5, in <module>\n",
      "    from impl.edges_eval_plot import edges_eval_plot\n",
      "  File \"/home/pramesh/teed/external/edge_eval/impl/edges_eval_plot.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 1273, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "  File \"/home/pramesh/teed/external/edge_eval/.venv/lib/python3.9/site-packages/matplotlib/__init__.py\", line 741, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('Agg')\n",
    "for i in range(7):\n",
    "    mat_dir = f\"checkpoints/CLASSIC/{i}/UDED_res/result_mat\"\n",
    "    call_ods_ois(\"CLASSIC\", mat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/0/0_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/0/0_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 77.967976.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/1/1_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/1/1_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 665.925018.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/2/2_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/2/2_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 701.964149.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/3/3_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/3/3_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 672.510843.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/4/4_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/4/4_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 698.732698.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/5/5_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/5/5_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 669.216861.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_35_165_rotation_true/CLASSIC/6/6_model.pth\n",
      "Restoring weights from: checkpoints_35_165_rotation_true/CLASSIC/6/6_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 707.646156.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/0/0_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/0/0_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 695.361899.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/1/1_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/1/1_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 648.391805.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/2/2_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/2/2_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 649.690074.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/3/3_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/3/3_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 677.060171.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/4/4_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/4/4_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 662.650669.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/5/5_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/5/5_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 680.624444.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_800_2800_rotation_false/CLASSIC/6/6_model.pth\n",
      "Restoring weights from: checkpoints_800_2800_rotation_false/CLASSIC/6/6_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 687.969362.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/0/0_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/0/0_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 684.529714.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/1/1_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/1/1_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 665.241781.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/2/2_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/2/2_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 723.673763.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/3/3_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/3/3_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 718.728315.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/4/4_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/4/4_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 700.338896.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/5/5_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/5/5_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 675.088619.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n",
      "Number of GPU's available: 1\n",
      "Pytorch version: 2.6.0+cu124\n",
      "Trainimage mean: [104.007, 116.669, 122.679, 137.86]\n",
      "Test image mean: [104.007, 116.669, 122.679, 137.86]\n",
      "output_dir: result/checkpoints_80_400_rotation_false/CLASSIC/6/6_model.pth\n",
      "Restoring weights from: checkpoints_80_400_rotation_false/CLASSIC/6/6_model.pth\n",
      "['01-0843x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['02-0868x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['03-35028.png']: torch.Size([1, 3, 488, 728])\n",
      "['04-0896x4.png']: torch.Size([1, 3, 512, 768])\n",
      "['05-WIREFRAME-2.png']: torch.Size([1, 3, 504, 752])\n",
      "['06-elephant_3.png']: torch.Size([1, 3, 512, 512])\n",
      "['07-CITYSCAPES-2C.png']: torch.Size([1, 3, 720, 720])\n",
      "['08-ADE20K-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['09-MDBD-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['10-196062.png']: torch.Size([1, 3, 488, 728])\n",
      "['11-NYUD-2.png']: torch.Size([1, 3, 640, 848])\n",
      "['12-cameraman.png']: torch.Size([1, 3, 536, 536])\n",
      "['13-BIPED-1C.png']: torch.Size([1, 3, 720, 720])\n",
      "['14-comic.png']: torch.Size([1, 3, 544, 376])\n",
      "['15-P1020854.png']: torch.Size([1, 3, 720, 960])\n",
      "['16-tire.png']: torch.Size([1, 3, 512, 512])\n",
      "['17-1194.png']: torch.Size([1, 3, 600, 600])\n",
      "['18-img_5935.png']: torch.Size([1, 3, 640, 848])\n",
      "['19-img_6150.png']: torch.Size([1, 3, 640, 848])\n",
      "['20-2009_003829.png']: torch.Size([1, 3, 504, 752])\n",
      "['21-00065305.png']: torch.Size([1, 3, 608, 752])\n",
      "['22-335094.png']: torch.Size([1, 3, 488, 728])\n",
      "['23-img_011_SRF_2_HR.png']: torch.Size([1, 3, 776, 424])\n",
      "['24-2010_002838.png']: torch.Size([1, 3, 752, 640])\n",
      "['25-2008_002622.png']: torch.Size([1, 3, 752, 536])\n",
      "['26-P1020177.png']: torch.Size([1, 3, 720, 960])\n",
      "['27-img_5264.png']: torch.Size([1, 3, 640, 848])\n",
      "['28-img_043_SRF_2_HR.png']: torch.Size([1, 3, 504, 768])\n",
      "['29-img_5182.png']: torch.Size([1, 3, 640, 848])\n",
      "['30-167062.png']: torch.Size([1, 3, 488, 728])\n",
      "******** Testing finished in UDED dataset. *****\n",
      "FPS: 697.345016.4\n",
      "-------------------------------------------------------\n",
      "TED parameters:\n",
      "58910\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.run_eval_all import run_eval_all\n",
    "run_eval_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
